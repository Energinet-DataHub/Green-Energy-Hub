name: Databricks Aggregation Job Infrastructure Deploy

on:
  #push:
    # branches:
    #   - main
    # paths:
    #   - build/terraform/aggregation_streaming_job/**
    #   - src/streaming/aggregation-jobs/**
    #   - .github/workflows/aggregation-job-cd.yml 
    workflow_dispatch:
      inputs:
        resource_group_name:
          description: 'Resource Group Name You Want to Use for Deployment'
          required: true
          default: ''
        env_name:
          description: 'Env name used to postfix resources names'
          required: true
          default: ''
        client_id:
          description: 'Service Principal Client (Application) Id'
          required: true
          default: ''
        object_id:
          description: 'Service Principal Object Id'
          required: true
          default: ''
        client_secret:
          description: 'Service Principal Secret'
          required: true
          default: ''

jobs:
  # Set the job key. The key is displayed as the job name
  # when a job name is not provided
  databricks_aggregation_job_deploy:
    # Name the Job
    name: Create Databricks Aggregation Job Infrastructure
    # Set the type of machine to run on
    # Resource Group set for automated trigger purposes, if ran manually it is overriden
    # All ENV Vars set bellow are needed for all the actions to run successfuly
    runs-on: ubuntu-latest
    env:
      WHEEL_VERSION: "1.0"
      WHEEL_STORAGE_ADDRESS: https://enrgtwheels.blob.core.windows.net/wheels/

    steps:
      
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Construct Wheel File Name
        uses: ./.github/actions/construct-wheel-file-name

      - name: Read Pipeline Configuration
        uses: ./.github/actions/read-pipeline-configuration

      - name: Set Environment Secrets
        run: |  
          echo "ARM_TENANT_ID=${{ secrets.TENANT_ID }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_ID=${{ secrets[env.GITHUB_SECRET_NAME_SPN_ID] }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_OBJECT_ID=${{ secrets[env.GITHUB_SECRET_NAME_SPN_OBJECT_ID] }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=${{ secrets[env.GITHUB_SECRET_NAME_SPN_SECRET] }}" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=${{ secrets[env.GITHUB_SECRET_NAME_SUBSCRIPTION_ID] }}" >> $GITHUB_ENV

      # Run only if workflow started on demand (manually triggered)
      - name: Set Variables from Pipeline Inputs
        uses: ./.github/actions/override_env_settings
        if:   github.event.inputs.resource_group_name
    
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1.2.1
        with:
          terraform_wrapper: false
      
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.7' # Version range or exact version of a Python version to use, using SemVer's version range syntax
          architecture: 'x64' # optional x64 or x86. Defaults to x64 if not specified

      - name: Configure Azure CLI
        uses: ./.github/actions/azure-cli-install-login

      - name: Obtain Databricks Workspace ID and Url
        id: obtain-db-id-url
        uses: ./.github/actions/obtain-databricks-id-url
        
      - name: Databricks CLI Install and Connect
        uses: ./.github/actions/databricks-cli-install-connect
        with:
          workspace-url: ${{ steps.obtain-db-id-url.outputs.workspace-url }}

      - uses: suisei-cn/actions-download-file@v1
        name: Download the Wheel File
        with:
          url: "${{ env.WHEEL_STORAGE_ADDRESS }}${{ env.WHEEL_FILE_NAME }}"
          target: wheels/

      - name: Copy Wheel File to Databricks Workspace
        uses: ./.github/actions/copy-wheel-file-to-databricks

      - name: Copy Job Definitions to DBFS
        run: |    
          dbfs cp --overwrite -r ./src/streaming/aggregation-jobs "dbfs:/streaming"

      - name: Set databricks_id TF Var  
        run: |
          echo "TF_VAR_databricks_id=${{ steps.obtain-db-id-url.outputs.workspace-id }}" >> $GITHUB_ENV 
      
      - name: Configure Terraform Backend
        uses: ./.github/actions/configure-terraform-backend
        with:
          backend-file-path: "./build/terraform/databricks_cluster/backend.tf"
          resource-group-name: "${{ env.RESOURCE_GROUP_NAME }}"
          storage-account-name: "enrgtfstate${{ env.ENV_NAME }}"

      # Create aggregation job cluster
      - name: Terraform Databricks Init
        working-directory: ./build/terraform/databricks_cluster
        run: terraform init
                  
      - name: Terraform Databricks Apply
        id: terraform-apply 
        working-directory: ./build/terraform/databricks_cluster
        run: |
          terraform apply -no-color -auto-approve
          echo "::set-output name=job-ids::$(terraform output databricks_job_ids)"
        continue-on-error: false